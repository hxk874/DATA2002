---
title: "Credit data"
subtitle: "Exhaustive Searches and stepwise methods"
format: html
---

```{r, warning=FALSE, message=FALSE}
library(ISLR)
library(sjPlot)
library(leaps)
library(lmSubsets)
```

## Exhaustive searches

Goal: predict the average credit card balance given the explanatory variables

```{r}
# ?ISLR::Credit
str(Credit)
```
```{r}
Credit %>%
  mutate(ID = factor(ID))
```

```{r}
#plot(Credit$)
```

```{r}
# lm(Balance ~ ID, data = Credit %>% )
```

### Using the `regsubsets()` function

```{r regsubsets-forward}
exhaustive.credit <- leaps::regsubsets(
  Balance ~ . - ID, # Balance against everything except ID
  data = Credit,
  method = "exhaustive", 
  nvmax = 11)
summary.exhaustive.credit <- summary(exhaustive.credit)
summary.exhaustive.credit
str(summary.exhaustive.credit)
```
Interpretation:
- "Model of size X, has the variables ..."
- Best model: optimizing the RSS
    In sample based
    will have the highest R squared
- If: compare models of different sizes: use AIC


#### AIC

For linear models, [Mallows Cp](https://en.wikipedia.org/wiki/Mallows%27s_Cp) will give the same result as the AIC.

$$BIC = n \log \left( \frac{RSS}{n} \right) + 2p$$

```{r regsubsets-forward-Cp}
# Which model gives the minimum cp? (/AIC)
min.cp <- which.min(summary.exhaustive.credit$cp)
plot(summary.exhaustive.credit$cp, type = 'l',
     ylab = "Mallows C_p measure", 
     xlab = "Number of features")
points(summary.exhaustive.credit$cp)
points(min.cp, summary.exhaustive.credit$cp[min.cp], pch = 4, col = "blue", cex = 2)
```
Interpretation: 6 gives the lowest cp


#### BIC

The [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion) is an alternative information criterion that tends to penalise more heavily than the AIC for extra predictors.

$$BIC = n \log \frac{RSS}{n} + \log n$$

```{r regsubsets-forward-BIC}
min.bic <- which.min(summary.exhaustive.credit$bic)
plot(summary.exhaustive.credit$bic, type = 'l',
     ylab = "BIC", 
     xlab = "Number of features")
points(summary.exhaustive.credit$bic)
points(min.bic, summary.exhaustive.credit$bic[min.bic], pch = 4, col = "blue", cex = 2)
```
Interpretation
- Best model of size 4: find the variables

Compare with the model of size 6 (from the AIC interpretation)


#### Adjusted R2
has a correction in it

```{r regsubsets-forward-}
max.adjr2 <- which.max(summary.exhaustive.credit$adjr2)
plot(summary.exhaustive.credit$adjr2, type = 'l',
     ylab = "Adjusted R2", 
     xlab = "Number of features")
points(summary.exhaustive.credit$adjr2)
points(max.adjr2, summary.exhaustive.credit$rsq[max.adjr2], pch = 4, col = "blue", cex = 2)
```

### Using the `lmSubsets()` function

```{r}
exhaustive.out = lmSubsets::lmSubsets(
  Balance ~ . - ID,
  data = Credit, nbest = 1, # the single best model of each model size
  # never do a model with more than 10 predictors
  nmax = NULL,
  method = "exhaustive")
plot(exhaustive.out, penalty = "BIC") # because it got that panalise parameter
plot(exhaustive.out, penalty = "AIC")
```
BIC goes up a litte more than the AIC

```{r}
# select the best
exhst.cp = lmSubsets::lmSelect(exhaustive.out, penalty = "AIC")
exhst.bic = lmSubsets::lmSelect(exhaustive.out, penalty = "BIC")
```

Compare the exhaustive Cp (AIC) with exhaustive BIC:

```{r}
sjPlot::tab_model(
  refit(exhst.cp), refit(exhst.bic), # extract the best models 
  show.ci = FALSE, show.p = FALSE,
  dv.labels = c("Exhaustive Cp", "Exhaustive BIC"))
```



## Stepwise

```{r}
M0 = lm(Balance ~ 1, data = Credit)
M1 = lm(Balance ~ . - ID, data = Credit)
```

### Backwards stepwise

The [BIC](https://en.wikipedia.org/wiki/Bayesian_information_criterion) is an alternative information criterion that tends to penalise more heavily than the AIC for extra predictors. For the AIC the penalty is $2p$ whereas for the BIC the penalty is $\log(n)p$.

```{r}
back_aic = step(M1, direction = "backward", trace = 0) # try to set trace=1 to see results underlying
summary(back_aic)
```

```{r}
back_bic = step(M1, direction = "backward", k = log(nrow(Credit)), trace = 0)
summary(back_bic)
```


### Forward stepwise

```{r}
# AIC
fwd_aic = step(M0, scope = list(lower=M0, upper=M1), 
              direction = "forward", trace = 0)
summary(fwd_aic)
```
```{r}
fwd_bic = step(M0, scope = list(lower=M0, upper=M1), 
               direction = "forward", k = log(nrow(Credit)), 
               trace = 0)
summary(fwd_bic)
```



### Forward and backward stepwise

```{r}
both_aic = step(M1, direction = "both", trace = 0)
summary(both_aic)
```

```{r}
both_bic = step(M1, direction = "both", k = log(nrow(Credit)), trace = 0)
summary(both_bic)
```


### Summary

```{r}
sjPlot::tab_model(
  back_aic, fwd_aic, both_aic,
  back_bic, fwd_bic, both_bic, 
  show.ci = FALSE, show.p = FALSE,
  dv.labels = c("Backward AIC", "Forward AIC", "Both AIC",
                "Backward BIC", "Forward BIC", "Both BIC"))
```

## Next steps...

Having identified some candidate models, you might go ahead and check their out of sample performance...
