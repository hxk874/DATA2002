---
title: "Lab"
output: html_document
date: "2024-09-12"
---

## Group work
```{r}
sample(1:6, 2, replace = TRUE)
sample(1:6, 2, replace = TRUE)
sample(1:6, 2, replace = TRUE)
sample(1:6, 2, replace = TRUE)
sample(1:6, 2, replace = TRUE)
sample(1:6, 2, replace = TRUE)
```

2. Using your individual sample, estimate the distribution of the sample mean using bootstrap resampling. Use 200 bootstrap resamples. Calculate a bootstrap confidence interval for the population mean average house price of the street.

```{r}
pop = c(2.1,	2.4,	5.9,	2.8,	2.9,	6.4)
```


```{r}
set.seed(12)
sample(pop, 6, replace= FALSE)
```

you cant use $t^*$, when you don't know the underlying dist, thus use bootstrapping instead. 


# Questions

## Speed of light

```{r}
library(tidyverse)
speed_file = read_csv("https://raw.githubusercontent.com/DATA2002/data/master/speed_of_light.txt")
speed = speed_file$Speed_of_Light

ggplot(speed_file, aes(x="", y = Speed_of_Light)) + 
  geom_boxplot(colour = "red", outlier.size = 2) + 
  theme_classic(base_size = 16) + 
  labs(x = "", y = "Speed") + coord_flip()
```

a. Generate one bootstrap sample and compare this sampled data with the original data.

Not a population data, this is a sample 
```{r}
true_mean = mean(speed) # 26.212

set.seed(123)
my_sam = sample(speed, 66, replace = TRUE)

# compare using a boxplot
boxplot(speed, my_sam, horizontal = TRUE)
```
Another way to compare, use a linear function based on a df:


b. Compute the mean and median of the bootstrap sample and compare with the corresponding values in the original data.
```{r}
length(speed)
(diff = speed-my_sam)
mean(my_sam)
mean(diff)
median(diff)
```




c. Draw another bootstrap sample and repeat the comparison. Repeat this 20 times and see if your conclusion changes. Inspect first ten bootstrap estimates of the mean. 
Visualise the result. Hint: Write a for loop (see example in lecture slides).
```{r}
bt_res = NULL

for(i in 1:20){
  bs_sample = sample(speed, replace = TRUE)
  bt_res[i] = mean(bs_sample)
}
bt_res[1:10]
```



d. Typically one draws a large number of bootstrap samples, say 1000 or more. 
Try different numbers of bootstrap samples and see how the shape of the histogram changes.




e. Find a 95% bootstrap confidence interval for the mean using the 2.5 and 97.5 percentiles as the confidence limits. 
Compare this with a “traditional” confidence interval that uses the $t$-distribution.


$$[\bar{x} - t^* \frac{s}{\sqrt{n}} , \bar{x} + t^* \frac{s}{\sqrt{n}} ]$$
```{r}
```

$$[26.212 - t^* \frac{s}{\sqrt{66}} , 26.212 + t^* \frac{s}{\sqrt{66}} ]$$



f. Generate 95% bootstrap confidence intervals calculation for the median and the MAD ('Median absolute deviation').



## Cotinine
single sample group

If the data follows a N, then we can do t-test
```{r}
x = c(0, 87, 173, 253, 1, 103, 173, 265, 1, 112, 198, 266, 3, 121, 
      208, 277, 17, 123, 210, 284, 32, 130, 222, 289, 35, 131, 227,
      290, 44, 149, 234, 313, 48, 164, 245, 477, 86, 167, 250, 491)
```

a. Calculate some simple descriptive measures of the data, construct a histogram and a Q-Q plot. Provide a brief description of the sample data.

```{r}
summary(x)
```
notice:
- mean and median pretty close
- 2 outliers - e.i. Max. => skewed to the right
    We should not remove them or ignore them


```{r}
# plot histogram and Q-Q plot
```
Not perfectly in proportions with N => permutation test 


b.Based on your descriptive summary of the data, do you think there are any outlying, or unusually large, observations that may impact upon any inferential test that you perform? In your description, take into consideration the summary statistics and histogram of the remaining data.


c. Using R and the complete sample, perform a standard $t$-test of the hypotheses $H_0 : \mu=130$ vs $H_1 : \mu \neq 130$. At the 5% level of significance, what can you conclude about the cotinine levels of the smokers in the population?

40 observations, df=39
```{r}
t.test(x, mu=130)
```



d. Perform a sign test to test $H_0 : \mu=130$ vs $H_1 : \mu \neq 130$.
```{r}
x1 = x[x != 130]
length(x)
bigger = x1 > 130
table(bigger)
```
Perform a binom test
15 negative differences
```{r}
# succes prob = 0.5

# we have two tailed: then 2x
2*(pbinom(15,30,))

# easier method
binom.test(table(bigger))
```


both the one-sample t-test and the permutation test, lets us reject the null hypothesiis, since the normality assumption is not heavaly violated


## Cereal

One of the variable `mfr` represents the manufacturer of cereal where 
  A = American Home Food Products, 
  G = General Mills, 
  K = Kelloggs, 
  N = Nabisco, 
  P = Post, 
  Q = Quaker Oats, 
  R = Ralston Purina.

Check the size of your data. 
Think about what the number of rows actually means.
```{r}
library(tidyverse)
cereal = read_csv("https://raw.githubusercontent.com/DATA2002/data/master/Cereal.csv", na = "-1")
# if you've downloaded it to your computer
# cereal = read_delim("Cereals.txt", delim = "\t", na = "-1")
## Looking at the start of the data
dplyr::glimpse(cereal)
```

1. Produce some basic summary statistics the nutrients “sugar” and “sodium”
```{r}
summary(cereal)
```

```{r}
summary(cereal$sugars)
boxplot(cereal$sugars, horizontal = TRUE)
```

```{r}
summary(cereal$sodium)
boxplot(cereal$sodium, horizontal = TRUE)
```


```{r}
cereal %>%
    drop_na(sugars, sodiium) 
    # use pivot to clean
# kable makes a pretty table in html
  
```

table1 funnction is handy for the same as above


2. Restricting attention to G = General Mills and K = Kellogg’s cereals, visualise the distribution of sodium content between the two manufacturers. 
Does it look like there is equal variance between the two groups? Could you safely assume normality within each group?

two-sample t-test


```{r}
cereal %>%
  filter(mfr in c("G", "K"))
```


Q-Q plot show us that the data from "G" in not on a staight line

3. Perform a permutation test to test whether there is a significant difference in the mean sodium content between the two manufacturers.

check if length of G and K is the same.

if perm to big, do random instead

wilcox:
  keep smae number of data
  shuffle around, 10000 rep
  
plot:
  red line: obs t0

abs(everything) --> new plot
-> new t0

first: how we would normally do it




# Quiz
```{r}
# Calculate the p-value for getting 3 or more correct out of 6 guesses
p_value <- 1 - pbinom(2, 6, 0.2)
p_value

```





```{r}
lung_vol = c(8.028, 7.189, 5.526, 8.594, 8.501, 9.620, 8.453, 5.106, 7.545, 5.247, 8.949, 8.969, 5.791, 6.601, 7.761)  
groups = c(rep("football", 8), rep("basketball", 7)) 
df = data.frame(groups = groups, lung_vol = lung_vol) 
```

```{r}
# Data setup
lung_vol = c(8.028, 7.189, 5.526, 8.594, 8.501, 9.620, 8.453, 5.106, 7.545, 5.247, 8.949, 8.969, 5.791, 6.601, 7.761)
groups = c(rep("football", 8), rep("basketball", 7))
df = data.frame(Group = groups, Lung_Volume = lung_vol)

# Split data by group
football_data <- df$Lung_Volume[df$Group == "football"]
basketball_data <- df$Lung_Volume[df$Group == "basketball"]

# Plotting
par(mfrow = c(1, 2)) # Set up the plotting area

# Q-Q plots
qqnorm(football_data, main = "Q-Q Plot for Football")
qqline(football_data, col = "steelblue", lwd = 2)

qqnorm(basketball_data, main = "Q-Q Plot for Basketball")
qqline(basketball_data, col = "steelblue", lwd = 2)



```

```{r}
boxplot(football_data, basketball_data, horizontal = TRUE, names = c("Football", "Basketball"))
```
```{r}
summary(football_data)
summary(basketball_data)
sd(football_data)
sd(basketball_data)
```



```{r}
# Load necessary library for Levene's Test
# Sum of Squares for Packaging and Total
sum_sq_packaging <- 1061.7
sum_sq_total <- 2597.5


# Calculate Sum of Squares for Residuals
sum_sq_residuals <- sum_sq_total - sum_sq_packaging

# Degrees of freedom for Packaging and Residuals
df_packaging <- 2
df_residuals <- 27

# Calculate Mean Squares
mean_sq_packaging <- sum_sq_packaging / df_packaging
mean_sq_residuals <- sum_sq_residuals / df_residuals

# Calculate F statistic
f_statistic <- mean_sq_packaging / mean_sq_residuals

# Return F statistic rounded to 1 decimal place
round(f_statistic, 3)

```

