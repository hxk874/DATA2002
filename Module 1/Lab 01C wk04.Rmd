---
title: "Lab 01C: Week 4"
author: "Ellen Ebdrup"
date: "2024-08-22"
output: 
  html_document: 
    ### IMPORTANT ###
    # self_contained: true # Creates a single HTML file as output
    code_folding: show # Code folding; allows you to show/hide code chunks
    ### USEFUL ###
    code_download: true # Includes a menu to download the code file
    ### OPTIONAL ###
    df_print: paged # Sets how dataframes are automatically printed
    theme: readable # Controls the font, colours, etc.
    toc: true # (Useful) Creates a table of contents!
    toc_float: true # table of contents at the side
    number_sections: false # (Optional) Puts numbers next to heading/subheadings
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE)
```

```{r, echo=FALSE}
library(tidyverse)
library(dplyr)

library(knitr)
library(kableExtra)
library(devtools)

library(ggplot2)

library(janitor)
library(lubridate)

library(stats)
library(mosaic)
```

# Quick Quiz

## TV violence

A study of the amount of violence viewed on television as it relates to the age of the viewer yields the results shown in the accompanying table for 81 people.
 
```{r, echo=FALSE}
# Define the data in a matrix format for clearer organization
data <- matrix(
  c(8, 12, 21, 18, 15, 7),
  nrow = 2,
  byrow = TRUE,
  dimnames = list(
    c("Low violence", "High violence"),
    c(" 16-34 ", " 35-54 ", " 55 and over ")
  )
)

data %>%
  kbl(caption = "Age", 
      row.names = TRUE) %>%
  kable_styling(
      full_width = F)
```

Does it look like there’s a significant relationship between age group and violence viewing preference? No need to do a test at this point, just consider the numbers, and the visualisations below.

```{r}
x = matrix(c(8, 18, 12, 15, 21, 7), ncol = 3)
colnames(x) = c("16-34", "35-54", "54+")
rownames(x) = c("Low violence", "High violence")
y = x |> as.data.frame() |> 
  tibble::rownames_to_column(var = "viewing") |> 
  tidyr::pivot_longer(cols = c("16-34", "35-54", "54+"), 
                      names_to = "age", values_to = "count")
p_base = ggplot(y, aes(x = age, y = count, fill = viewing)) + 
  theme_bw(base_size = 12) + 
  scale_fill_brewer(palette = "Set1") + 
  labs(fill = "", x = "Age group") +
  theme(legend.position = "top")
p1 = p_base + 
  geom_bar(stat = "identity") + 
  labs(y = "Count") 
p2 = p_base + 
  geom_bar(stat = "identity", position = "fill") + 
  labs(y = "Proportion")
gridExtra::grid.arrange(p1, p2, ncol = 2)
```


## Income and IQ

103 children attending a pre-school were classified by parents’ income group and by IQ (intelligence quotient).

```{r, echo=FALSE}
x = matrix(c(14, 25, 23, 18, 8, 15), 
           ncol = 2)

colnames(x) = c("High IQ", "Moderate/low IQ")
rownames(x) = c("A", "B", "C")

x %>%
  kbl() %>%
  kable_styling(full_width = F)
```

Does it look like the fractions of IQ differ significantly in the three income groups? No need to do a test at this point, just consider the observed counts, and the visualisations below.

```{r}
y = x |> as.data.frame() |> 
  tibble::rownames_to_column(var = "income") |> 
  tidyr::pivot_longer(c("High IQ", "Moderate/low IQ"), 
                      names_to = "iq", values_to = "count")
p_base = ggplot(y, aes(x = income, y = count, fill = iq)) + 
  theme_bw(base_size = 12) + 
  scale_fill_brewer(palette = "Set1") + 
  labs(fill = "", x = "Income group") +
  theme(legend.position = "top")
p1 = p_base + 
  geom_bar(stat = "identity") + 
  labs(y = "Count") 
p2 = p_base + 
  geom_bar(stat = "identity", position = "fill") + 
  labs(y = "Proportion")
gridExtra::grid.arrange(p1, p2, ncol = 2)
```


# Exercises

## Personality type

A psychologist is interested in testing whether there is a difference in the distribution of personality types for business majors and social science majors. She performs a personality test on a random sample of 258 business students and a random sample of 355 social science students. The results of the study are shown in the table below. 

1. Visualise the data. 


2. What is the appropriate test in this context? [I.e. a test of goodness of fit, homogeneity or independence.] 

3. Perform the test using a 5% level of significance.


```{r}
counts = c(41, 52, 46, 61, 58, 72, 75, 63, 80, 65)
c_mat = matrix(counts, nrow = 2, byrow = TRUE)
colnames(c_mat) = c("Open", "Conscientious", "Extrovert", "Agreeable", "Neurotic")
rownames(c_mat) = c("Business", "Social Science")
```

```{r}
c_mat %>%
  kbl() %>%
  kable_styling(full_width = F)
```


1. Visualise the data. 

```{r}
counts = c(41, 52, 46, 61, 58, 72, 75, 63, 80, 65)
c_mat = matrix(counts, nrow = 2, byrow = TRUE)
colnames(c_mat) = c("Open", "Conscientious", "Extrovert", "Agreeable", "Neurotic")
rownames(c_mat) = c("Business", "Social Science")

# very simple display method, where you don't have to convert into a df first.
par(mar=c(0,1,1,0)) # controls the margins of this base R plot
mosaicplot(t(c_mat), main = NULL)
```
In a mosaic plot, the lengths of the rectangles across are proportional to the number of cells across both the x and the y axes. It’s a generalisation of a stacked bar chart where the bars lengths are proportional to the number in each category.

To visualise using ggplot2, we need to first create a data frame:
```{r}
# convert into a df to display.
data.frame(
  counts = counts,
  major=c(rep("Business",5), rep("Social Science",5)), 
  personality= rep(c("Open", "Conscientious", "Extrovert", "Agreeable", "Neurotic"), 2))

df <- as.data.frame(c_mat)
df <- df |>
  tibble::rownames_to_column("major") |> 
  tidyr::pivot_longer(cols=!major, # columns that are not the "major" column
                      names_to = "personality",
                      values_to = "counts")
```
We can now use this as the input to the `ggplot()` function to generate either a bar plot representing the counts or a bar plot representing the proportions.

```{r}
df |> ggplot() + 
  aes(x = major, y = counts, fill = personality) + 
  geom_col() + 
  scale_fill_brewer(palette = "Set1") + 
  coord_flip() + 
  labs(y = "Number of students", x = "Major", fill = "Personality type") + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  theme(legend.position = "top")
```

```{r}
df |> ggplot() + 
  aes(x = major, y = counts, fill = personality) + 
  geom_col(position = "fill") + 
  scale_fill_brewer(palette = "Set1") + 
  coord_flip() + 
  labs(y = "Percent of students", x = "Major", fill = "Personality type") +
  scale_y_continuous(labels = scales::percent) + 
  guides(fill = guide_legend(reverse = TRUE)) + 
  theme(legend.position = "top")
```


2. What is the appropriate test in this context? I.e. a test of goodness of fit, homogeneity or independence. Perform the test using a 5% level of significance.

The most appropriate test here is a test for homogeneity because we have sampled from two different populations (the population of business students and the population of social science students).

```{r}
# check: expected must always be greater than 5:
chisq.test(c_mat)$expected |> round(1)
```

```{r}
# Perform chi^2 test
chisq.test(c_mat)
```

### Workflow

1. Hypothesis

$H_0$ : The distribution of personality types is the same for both majors

$H_1$ : The distribution of personality types is not the same for both majors

2. Assumption

- Observations are randomly selected: $e_ i =np_i\ge5 \ \ \text{for all} \ i$

Confirmed by calculating the expected cell counts 

- Independent observations.

Confirmed as we are told there was random sampling from each population.


3. Test Statistic
$$T = \sum_{i=1}^{r} \sum_{j=1}^{c} \frac{(Y_{ij}-e_{ij})^2}{e_{ij}} \sim \chi_{(r-1)(c-1)}^2 \ \ \text{ Under } \ H_0$$

4. Observed Test Statistic, $t_0 = 3.006$
$$t_0 = \sum_{i=1}^{2} \sum_{j=1}^{5} \frac{(y_{ij}-e_{ij})^2}{e_{ij}} \sim \chi_{1 \times 4}^2 \ \ \text{ Under } \ H_0 = 3.006$$

5. P-value
$$P(T\ge t_0) = P(\chi_{4}^2 \ge 3.006)=0.5568$$
How to find the p-value:
```{r}
# calculate the p-value manually either
1 - pchisq(q = 3.006, df = 4)

# or equivalently
pchisq(q = 3.006, df = 4, lower.tail = FALSE)
```


6. Decision

Since the p-value much greater than 0.05, we do not reject $H_0$. There is insufficient evidence to conclude that the distribution of personality types is different for business and social science majors. Another way of saying this is: the data are consistent with the null hypothesis that the distribution of personality types is the same across business and social science majors.


## Shocking

A psychological experiment was done to investigate the effect of anxiety on a person’s desire to be alone or in company.

A group of 30 subjects was randomly divided into two groups of sizes 13 and 17.

The subjects were all told that they would be subject to electric shocks.

* The “high anxiety” group was told that the shocks would be quite painful
* The “low anxiety” group was told that they would be mild and painless

Both groups were told that there would be a 10 minute wait before the experiment began and each subject was given the choice of waiting alone or with other subjects.

The results were as follows:

```{r}
x = matrix(c(12, 4, 16, 5, 9, 14, 17, 13, 30), ncol = 3)
colnames(x) = c("Togehter", "Alone", "Total") # D
rownames(x) = c("High", "Low", "Total") # R
df <- data.frame(x)

df %>%
  kbl() %>%
  kable_styling(full_width = F)
```

1. If we’re picking between homogeneity and independence, which is more appropriate here?

In this example, we started with one population, but then we stratified by anxiety, so in a sense we have **two groups** (sub-populations), one where we told them the shock would be quite painful and the other where we told them it would be mild. In this context it is more like a test for **homogeneity** where 
$$H_0 : \text{the proportion of people who choose to wait alone is the same in both groups and the proportion of people who choose to wait together is the same in both groups}$$

2. At the 5% level of significance perform each of the following tests:

i)   Fisher’s exact test
ii)  A chi-squared test without a continuity correction
iii) A chi-squared test with a continuity correction.
iv)  A chi-squared test using a Monte Carlo p-value (i.e. using simulation).

3. Do the results of the different tests agree? Which are you most convinced by?

4. Would it make sense to calculate a relative risk here? Calculate the odds ratio, confidence interval and provide an interpretation.



2 Perform test
At the 5% level of significance perform each of the following tests:

```{r}
counts = c(12, 5, 4, 9)
c_mat = matrix(counts, ncol = 2, byrow = TRUE)
colnames(c_mat) = c("Together", "Alone")
rownames(c_mat) = c("High", "Low")
c_mat
```


#### Fisher’s exact test

When do we use it?

👉 When the $e_{ij}<5$ for at least one cell (or a sample size is small)

Procedure:
\
1. Consider all possible contingency tables
\
2. Enumerate all tables as or more extreme than the observed contingency table **while fixing row and column totals**
\
3. For each of these **more extreme** tables we obtained in step 2, calculate the probability of observing that table
\
4. Sum the probabilities up to get the exact p-value

```{r}
fisher.test(c_mat)
```
p-value = 0.0634 > 0.05 => we don't reject $h_0$

####  A chi-squared test without a continuity correction

```{r}
chisq.test(c_mat, correct = FALSE)
```
p-value = 0.0303 < 0.05 => we reject $h_0$



#### A chi-squared test with a continuity correction.
```{r}
chisq.test(c_mat, correct = TRUE)
```
p-value = 0.07 > 0.05 => we don't reject $h_0$

#### A chi-squared test using a Monte Carlo p-value (i.e. using simulation).

Why do we use it?

👉 **Non-parametric**: no assumption about the underlying distribution of the population is required

1. Resample the original contingency table with replacement lots of times (B times), while keeping row/column totals the same
\
2. For each of these resampled contingency tables, calculate a (resampled) test statistic
\
3. Draw a distribution of these B many resampled test statistics
\
4. Calculate the proportion of resampled test statistics as or more extreme than the test statistic calculated from the original contingency table to get a Monte Carlo p-value

```{r}
set.seed(1)
chisq.test(c_mat, simulate.p.value = TRUE, B = 20000)
```
p-value = 0.064 > 0.05 => we don't reject $h_0$

```{r}
# if you want to do the simulation "manually":
# 1. extract the test statistic from the original data
test_stat = chisq.test(c_mat, correct = FALSE)$statistic
# 2. generate 20000 tables with the same margins as the
#    observed data
set.seed(2002)
rand_tables = r2dtable(n = 20000, r = rowSums(c_mat), c = colSums(c_mat))
# 3. calculate the the test statistic (without a continuity correction)
#    for each of the randomly generated tables.
#    Notes: lapply() applys a function to each of the elements in a list
#           unlist() takes a list and converts it to a vector
sim_stats = unlist(lapply(rand_tables, 
                          function(x) chisq.test(x, correct = FALSE)$statistic))
#    You could also do this using a for loop
# 4. have a look at the distribution of the test statistics
#    that were generated under the null hypothesis of independence
hist(sim_stats, breaks = 30)
```

```{r}
# 5. calculate the Monte Carlo p-value as the proportion of 
#    simulated test statistics that are more extreme than 
#    the text statistic that we observed.
mean(sim_stats >= test_stat)
```

p-value = 0.0657 > 0.05 => we don't reject $h_0$

### Conclusion

3. Do the results of the different tests agree? Which are you most convinced by?

Only the chi-squared test without the continuity correction gave a p-value that was less than 0.05. We’re more convinced by the other approaches which give more reliable results, particularly when the sample sizes are small. The Monte Carlo p-value is very similar to Fisher’s exact test (these would be our most preferred solutions) while the p-value for a chi-squared test with continuity correction is slightly larger.

For the odds ratio,

- NOT ii) and iii) chi
- The corrects is i) Fisher's! almost always go with it.


4. Would it make sense to calculate a relative risk here? Calculate the odds ratio, confidence interval and provide an interpretation.


```{r}
mosaic::oddsRatio(c_mat, verbose = TRUE)
```
If the 95% CI for an odds ratio does not include 1.0, then the odds ratio is considered to be statistically significant at the 5% level.

OR => significant

In this example we have sampled from the two groups (i.e. we fixed the number in the high group and we fixed the number in the low group), so it makes sense to estimate the conditional probabilities $P$(Together | High) and $P$(Together | Low).


If we interpret this output,


**Relative risk**:

* `Prop. 1: 0.7059` is our estimate of $P$(Together | High), the proportion of subjects who preferred to wait together in the high anxiety group (12/(12+5)).

* `Prop. 2: 0.3077` is our estimate of $P$(Together | Low), is the proportion of subjects who preferred to wait together in the low anxiety group (4/(4+9)).

* Relative risk: is the ratio of these two conditional probabilities, 
$$\text{Rel. Risk} 0.4359 = \text{(Prop. 2)/(Prop. 1)} = 0.3077 / 0.7059 = 0.44$$
This is different to what we would have done from the lecture, where we would have calculated (Prop. 1)/(Prop. 2) = 0.7059 / 0.3077 = 2.3. Either way is OK so long as we adjust the interpretation.
    * For the 2.3 relative risk, we’re saying that subjects who were told that it would be a painful shock were 2.3 times more likely to wait together than subjects who were told it wouldn’t be painful.
    * For the 0.44 relative risk, we’re saying that subjects who were told that it would not be a painful shock were 0.44 times more likely (i.e. they were less likely) to wait together than subjects who were told it would be painful.

**Odds Ratio**:

* `Odds 1: 2.4` is our estimate of $P$(Together | High) = $P$(Together | High) /$P$(Alone | High), the odds of subjects who preferred to wait together in the high anxiety group to (0.7059/(1-0.7059)).

* `Odds 2: 0.4444` is our estimate of $P$(Together | Low) = $P$(Together | Low) / $P$(Alone | Low), the odds of subjects who preferred to wait together in the low anxiety group to (0.3077/(1-0.3077)).

* Odds Ratio: `0.1852 = Odds 2/Odds 1 = 0.4444/2.4`. 
If we were following the approach in the lecture slides we would have calculated `Odds 1/Odds 2 = 2.4/0.4444 = 5.4`. Either way we just need to adjust our interpretation.

    * For the 5.4 odds ratio, we’re saying that the odds of waiting together for the painful shock group are 5.4 times the odds of waiting together for the mild shock group.

    * For the 0.19 odds ratio, we’re saying that the odds of waiting together for the mild shock group are 0.19 times the odds of waiting together for the painful shock group.
    
**Comparing to the null hypothesis**:

The null hypothesis is that the odds ratio is equal to 1 (no association). The 95% confidence interval for the odds ratio, (0.0384, 0.8932) does not contain 1, therefore we would reject the null hypothesis. HOWEVER, remember that the calculation of the confidence interval for the odds ratio, relied on similar assumptions to the chi-squared test, i.e. we need “reasonably large” sample sizes in each of the cells (can think of this as the expected cell counts of at least 5 assumption).

Note: to get the same values as we would have calculated in lectures, we just need to flip the rows in the table:
```{r}
mosaic::oddsRatio(c_mat[2:1,], verbose = TRUE)
```


## Asbestos fibres 

One of the breakthroughs that demonstrated the dangers to the exposure of asbestos is due to a study undertaken in the 1960’s (data reported in Selikoff (1981)). Chest x-rays of a sample of 1117 workers in New York were taken to determine the damage done due to the occupational exposure of the workers to asbestos fibres. These workers were classified according to their years of exposure to the fibres and the severity of asbestosis that they were diagnosed with. The data appear in the following contingency table

```{r}
asbestos = matrix(c(310, 212, 21, 25, 7, 36, 158, 35, 102, 
                    35, 0, 9, 17, 49, 51, 0, 0, 4, 18, 28), nrow = 5)
colnames(asbestos) = c("None", "Grade 1", "Grade 2", "Grade 3")
rownames(asbestos) = c("0-9", "10-19", "20-29", "30-39", "40+")
df <- data.frame(asbestos)

df %>%
  kbl() %>%
  kable_styling(full_width = F)
```


```{r}
y = asbestos %>% 
  as.data.frame() %>% # convert the matrix to a dataframe
  tibble::rownames_to_column(var = "years") %>%  # treat row names (year group) as a column
  tidyr::gather(key = grade, value = count, -years) # convert the dataframe from a long format to a wide format (equivalent to pivot_longer)

# treat the grade variable as a factor variable instead of character variable
y$grade = factor(y$grade, levels = c("None", "Grade 1", "Grade 2", "Grade 3"), ordered = TRUE)

y %>% 
  ggplot() +
  aes(x = years, y = count, fill = grade) +
  geom_bar(stat = "identity") + # when you use a count variable (numbers) as a y variable, you have to set stat = "identiy"
  theme_grey(base_size = 16) + # change the theme just for aesthetics (base font size = 16)
  scale_fill_brewer(palette = "Set1") + # use a built-in R colour palette "Set1" just for aesthetics
  labs(fill = "", y = "Count", x = "Occupational exposure (yrs)") # relabel the plot
```

1. Adapt the **ggplot2** code above such that the y-axis is a proportion within each exposure length group. Does it look like there’s a relationship between the two variables?

```{r}
y = asbestos |> as.data.frame() |> 
  tibble::rownames_to_column(var = "years") |> 
  tidyr::gather(key = grade, value = count, -years)

y$grade = factor(y$grade, levels = c("None", "Grade 1", "Grade 2", "Grade 3"), ordered = TRUE)
ggplot(y, aes(x = years, y = count, fill = grade)) + 
  geom_bar(stat = "identity", position = "fill") +  
  theme_grey()  + 
  scale_fill_brewer(palette = "Set1") + 
  labs(fill = "", y = "Count", x = "Occupational exposure (yrs)")
```



2. Use the function `chisq.test()` to perform a standard chi-squared test of **independence** to determine whether there exists a statistically significant association between years of exposure to asbestos fibres and the severity of asbestosis that they were diagnosed with.


|      |  None  |  Grade 1  |  Grade 2  |  Grade 3  |  Row Total  |
|:----:|:----:|:----:|:----:|:----:|:----:|
| **0-9** | $p_{11}$ | $p_{12}$ | $p_{13}$ | $p_{14}$ | $p_{1\bullet}$ |
| **10-19** | $p_{21}$ | $p_{22}$ | $p_{23}$ | $p_{24}$ | $p_{2\bullet}$ |
| **20-29** | $p_{31}$ | $p_{32}$ | $p_{33}$ | $p_{34}$ | $p_{3\bullet}$ |
| **30-39** | $p_{41}$ | $p_{42}$ | $p_{43}$ | $p_{44}$ | $p_{4\bullet}$ |
| **40 +** | $p_{51}$ | $p_{52}$ | $p_{53}$ | $p_{54}$ | $p_{5\bullet}$ |
| **Column Total** | $p_{\bullet 1}$ | $p_{\bullet 2}$ | $p_{\bullet 3}$ | $p_{\bullet 4}$ | $p_{\bullet\bullet}=1$ |

where $p_{i\bullet}=\sum_{j=1}^{4}p_{ij}$ and $p_{\bullet j}=\sum_{i=1}^{5}p_{ij}$

The expected counts are:
```{r}
chisq.test(asbestos)$expected %>%
    round(1) %>%
  kbl() %>%
  kable_styling(full_width = F)
```

**Assumptions**  
Notice that cell (20-29, Grade 3) = 3.4 < 5. This violates the $e_i = np_i \ge 5 \forall i$

However, an alternative approach is to perform a permutation test, where we still use the test statistic but we no longer compare it to a chi-squared distribution, rather we resample the data in such a way that we know the rows and columns are independent and assuming the marginal totals of the contingency table are fixed.

But if we were to, then we would reject the $H_0$ since the p-value = 0 < 0.05. 
```{r}
chisq.test(asbestos)
```
**Workflow**

1. Hypothesis

$$H_0: p_{ij}=p_{i\bullet}p_{\bullet j}\ \ \text{for}\ i = 1,\ 2,\, 3,\ 4,\ 5 \ \ \text{and}\ \  j=1,\ 2,\ 3,\ 4 \ \ \ \text{vs} \ \ \ H_1: \text{The occupational exposure is not independent of the asbestos grade diagnosed}$$

2. Assumption

$$e_{ij}=\frac{y_{i\bullet} \times y_{\bullet j}}{n}\ge5 \ \ \text{for all} \ i, \ j$$

3. Test Statistic

$$T=\sum_{i=1}^{5}\sum_{j=1}^{4}{\frac{(Y_{ij}-y_{i\bullet}y_{\bullet j}/n)^2}{y_{i\bullet}y_{\bullet j}/n}}\sim\chi_{(5-1)(4-1)}^2 \ \ \text{Under} \ H_0$$

4. Observed Test Statistic

$$t_0=\sum_{i=1}^{5}\sum_{j=1}^{4}{\frac{(Y_{ij}-e_{ij})^2}{e_{ij}}}\sim\chi_{(5-1)(4-1)}^2=648.81\ \ \text{Under} \ H_0$$

5. P-value

$$P(T\ge t_0) = P(\chi_{12}^2\ge648.81) = 0$$

6. Decision

The chi-squared test returns a very small p-value. Hence, there is evidence to suggest that a statistically significant association exists between exposure to asbestos fibres and the severity of asbestosis that a worker is diagnosed with.



3. Use `x = r2dtable(____)` to randomly generate a contingency table with the same row and column totals as `asbestos`. Perform a chi-squared test and extract the test statistic using `chisq.test(x[[1]])$statistic`.

```{r}
row_totals = rowSums(asbestos)
row_totals
```

```{r}
col_totals = colSums(asbestos)
col_totals
```

Now we can use the `r2dtable()` function to randomly generate a contingency table with the same row and column totals:

```{r}
set.seed(2018)
rnd = r2dtable(n = 1, r = row_totals, c = col_totals) # randomly resample the original contingency table only once
chisq.test(rnd[[1]])$statistic # a resampled t_0 calculated from the randomly resampled contingency table above.
```


4. By using the `r2dtable()` function, perform a Monte-Carlo simulation to determine the p-value for the chi-squared test of independence. Generate 10,000 bootstrap resamples. Note: if doing this in an Rmd script, you might want to wrap your `chisq.test(___)$statistic` in `suppressWarnings()` so they don’t slow down your computer, e.g. `suppressWarnings(chisq.test(___)$statistic)`. Plot a histogram of your Monte Carlo test statistics.

The Monte-Carlo p-value obtained by generating 10,000 contingency tables, computing the chi-squared test statistic for each table and seeing the proportion of these exceed the observed test statistic.

```{r}
# Monte Carlo simulation with 10000 times

B = 10000 # 10,000 contingency tablesv
stat = numeric(length = B) # initialise an empty vector that will store resampled test statistics later
set.seed(2002)
tables = r2dtable(n = B, r = row_totals, c = col_totals) # randomly resample the original contingency table 10000 times

# for each of the 10000 resampled contingency tables, calculate a test statistic, extract it, and store it in the stat vector
# method 1: use a for loop
for (i in 1:B) {
    stat[i] = suppressWarnings(chisq.test(tables[[i]], )$statistic) 
}

# alternative approach (method 2)
# apply a function that extracts a test statistic to each of the resampled contingency tables in the "tables" R object
stat = sapply(tables, function(x) suppressWarnings(chisq.test(x)$statistic))

# calculate the Mote Carlo p-value
mc_pval = mean(stat >= t0)
mc_pval
```

We can look at the distribution of test statistics:
```{r}
# distribution of 10000 test statistics
hist(stat, xlab = "Test Statistics")
# we can see that the proportion of test statistics, as or more extreme than the observed test statistic (648.81) is almost 0
```

There are no permutation test statistics that were more extreme than the test statistic we observed on our original data. From the histogram, we can see that the observed test statistic, 648.8 is way past the range that we would expect to see if the null hypothesis of independence was true.

5. Use the `chisq.test()` function to perform a Monte-Carlo simulation that obtains a p-value. Do so using 10,000 bootstrap resamples.

The `chisq.test()` function can do all this for us:
```{r}
# Monte Carlo simulation with the chisq.test function
chisq.test(asbestos, simulate.p.value = TRUE, B = B)
```
