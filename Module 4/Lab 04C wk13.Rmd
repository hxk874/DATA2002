---
title: "Week 13 Lab"
date: "`r Sys.Date()`"
author: "Tutor: Sanghyun Kim"
output: 
  html_document: 
    ### IMPORTANT ###
    # self_contained: true # Creates a single HTML file as output
    code_folding: hide # Code folding; allows you to show/hide code chunks
    ### USEFUL ###
    code_download: true # Includes a menu to download the code file
    ### OPTIONAL ###
    df_print: paged # Sets how dataframes are automatically printed
    theme: readable # Controls the font, colours, etc.
    toc: true # (Useful) Creates a table of contents!
    toc_float: true # table of contents at the side
    number_sections: false # (Optional) Puts numbers next to heading/subheadings
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(message = FALSE, warning = FALSE)
```

```{r}
library(tidyverse)
```

# Who has diabetes?

```{r}
pima = readr::read_csv("https://raw.githubusercontent.com/DATA2002/data/master/pima.csv")
glimpse(pima)
```

## K-nearest neighbours

![](knn.png)

ðŸ™‚ Pros

- kNN is easy to understand and implement

- Analytically tractable and simple implementation

- Performance improves as the sample size grows

- Uses local information, and is highly adaptive

ðŸ™ Cons

- The distance metric only makes sense for quantitative variables (not categorical predictors).

- Does not scale well and predictions can be slow: computationally intensive when predicting new observations, data is processed at that time, lazy algorithm.

- Curse of dimensionality: doesnâ€™t perform well with high dimensional inputs (i.e. with lots of predictors)

- Easy to overfit/underfit: Small $k$ can overfit the data. High $k$ tend to â€œsmooth outâ€ the predictions, but if $k$ is too high then might underfit.

### 1

```{r}
library(class)
library(caret)
library(cvTools)
set.seed(1)
# an alternative scaling function that could be used is:
# norm_fn = function(x) { (x - min(x))/(max(x) - min(x)) }

pima_scaled = pima %>% 
  dplyr::mutate(
    dplyr::across(c(bmi, bp, glu, serum, skin), # For any of these variables
                  .fns = ~ dplyr::na_if(., 0)) # treat 0 values as NA
  ) %>% 
  drop_na() %>% # drop NA
  mutate(y = factor(y)) %>% # convert y to binary data
  mutate(across(where(is.numeric), .fns = scale)) # standardise all numeric variables

X = pima_scaled %>% select(-y) # get a dataframe of predictors
y = pima_scaled %>% select(y) %>% pull() # get a vector of the response variable (i.e., y)
n = length(y) # number of observations

knn5 = class::knn(train = X, test = X, cl = y, k = 5) # build a knn classifier with k = 5
caret::confusionMatrix(knn5, y)$table # confusion matrix
```

```{r}
caret::confusionMatrix(knn5, y)$overall[1]
```

Out of sample:

```{r}
K = 5  # number of CV folds
cvSets = cvTools::cvFolds(n, K)  # permute all the data, into 5 folds
glimpse(cvSets)
```

```{r}
cv_acc = NA # initialise results vector

for (j in 1:K) { # for each fold
    test_id = cvSets$subsets[cvSets$which == j] # extract row indices for test data
    X_test = X[test_id, ] # test predictors
    X_train = X[-test_id, ] # training predictors
    y_test = y[test_id] # test response
    y_train = y[-test_id] # training response
    fit = class::knn(train = X_train, test = X_test, cl = y_train, k = 5) # build a knn classifier (k = 5) with training data
    cv_acc[j] = caret::confusionMatrix(fit, y_test)$overall[1] # evaluate the knn classifier on the test data and extract accuracy
}
mean(cv_acc) # average accuracy
```

The out of sample accuracy is 0.77, which is quite a bit lower than the in-sample accuracy of 0.84.

### 2

```{r}
# range of k in knn to consider
res = data.frame(k = 1:50, cv_rep_acc = NA) # initialise a dataframe of accuracy values for k = 1, 2, ..., 50

for (i in res$k) { # for each k which represents number of nearest neighbours for the knn classifier
    cvSets = cvTools::cvFolds(n, K)  # permute all the data, into 5 folds
    cv_acc = NA  # initialise results vector
    
    for (j in 1:K) { # here k is the number of folds for cross-validation
        test_id = cvSets$subsets[cvSets$which == j] # extract row indices for test data
        X_test = X[test_id, ] # test predictors
        X_train = X[-test_id, ] # training predictors
        y_test = y[test_id] # test response
        y_train = y[-test_id] # training response
        fit = class::knn(train = X_train, test = X_test, cl = y_train, k = i) # train a knn classifier using a different number of nearest neighbours (k = i)
        cv_acc[j] = caret::confusionMatrix(fit, y_test)$overall[1] # evaluate the knn classifier on the test data and extract accuracy
    }
    res$cv_rep_acc[i] = mean(cv_acc) # for each k = i (number of nearest neighbours), calculate CV-average accuracy and store in the res dataframe.
}

res %>%
  ggplot() + 
  aes(x = k, y = cv_rep_acc) + 
  geom_point() + 
  geom_line() +
  geom_vline(xintercept = which.max(res$cv_rep_acc), colour = "red")
```

Looks like thereâ€™s a maximum around 30.

### 3

```{r}
## 5-fold repeated CV with 10 repeats
fitControl = trainControl(method = "repeatedcv", 
                          number = 5,
                          repeats = 10)
knnFit1 = train(y ~ ., 
                data = pima_scaled, # standardised dataframe
                method = "knn", 
                trControl = fitControl,
                tuneLength = 10)
knnFit1
```

```{r}
knnFit1$results
```

```{r}
knn_acc = max(knnFit1$results$Accuracy)
```

## Comparison

### 4

```{r}
set.seed(2021)
fitControl = trainControl(method = "repeatedcv", 
                          number = 5, 
                          repeats = 10)
# decision tree
rpartFit1 = train(y ~ ., data = pima_scaled, 
                  method = "rpart", 
                  trControl = fitControl)
rpart_acc = max(rpartFit1$results$Accuracy)

# random forests
rfFit1 = train(y ~ ., data = pima_scaled,
               method = "rf", 
               trControl = fitControl) 
rf_acc = max(rfFit1$results$Accuracy)

# logistic regression, full model (no selection has been done)
glmFit1 = train(y ~ ., data = pima_scaled, 
                method = "glm", family = "binomial", 
                trControl = fitControl)

# logistic regression, stepwise model (see Lab 4b)
glm_stepFit1 = train(y ~ npreg + glu + bmi + ped, 
                     data = pima_scaled, 
                     method = "glm", family = "binomial", 
                     trControl = fitControl)
glm_acc = glmFit1$results$Accuracy
glm_step_acc = glm_stepFit1$results$Accuracy
```

Comparing the results

```{r}
knn_acc
```

```{r}
rpart_acc
```

```{r}
rf_acc
```

```{r}
glm_acc
```

```{r}
glm_step_acc
```

```{r}
res = resamples(list(knn = knnFit1, 
                     tree = rpartFit1, 
                     forest = rfFit1,
                     `logistic full` = glmFit1, 
                     `logistic step` = glm_stepFit1))

ggplot(res) + 
  labs(y = "Accuracy")
```

A single decision tree performs worst, followed by knn. Logistic regression performed best (even better when some initial model selection was performed), closely followed by random forest.

### 5

The logistic regression and the random forest performed similarly well. I (Garth) have a preference towards interpretable and transparent models, and so given the similar performance, I would choose the logistic regression.

### 6

Not really. Our data only considered females at least 21 years old of Pima Indian heritage. If we were implementing it as a diagnostic tool, itâ€™s only been validated against the similar individuals. In order to extend it more broadly we would need to assess its predictive power on different populations (i.e. people of other heritages).


# Violent crime rates by states

## Hierarchical clustering

```{r}
dplyr::glimpse(USArrests)
```

### 1

```{r}
us_arrest_dist = dist(scale(USArrests))
```

### 2

```{r}
us_arrest_hc = hclust(us_arrest_dist)
plot(us_arrest_hc, hang = -1)
```

### 3

```{r}
# group into k = 4
us_arrest_groups = cutree(us_arrest_hc, k = 4) # cut the tree such that we have 4 clusters

us_arrest = USArrests %>%
    tibble::rownames_to_column() %>%
    mutate(hc4 = factor(us_arrest_groups)) # treat the classes as categorical data

p1 = us_arrest %>%
  ggplot() +
  aes(x = UrbanPop, y = Murder, colour = hc4) +
  geom_point() +
  theme_bw() +
  labs(title = "Hierarchical clusters")
p1
```

## K-means

```{r}
us_arrests_kmeans = kmeans(scale(USArrests), centers = 4)

us_arrest = us_arrest %>%
    mutate(km4 = factor(us_arrests_kmeans$cluster))

p2 = us_arrest %>%
  ggplot() + 
  aes(x = UrbanPop, y = Murder, colour = km4) + 
  geom_point() +
  theme_bw() + 
  labs(title = "k-means clusters")

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

In this case, the k-means and the hierarchical clustering give very similar results when we ask for 4 clusters from both. Note that the group labelling is arbitrary in the colouring (above) or the group labelling below.

```{r}
us_arrest %>%
  janitor::tabyl(hc4, km4)
```

# Who has diabetes? [Revisited]

## Clustering and dimension reduction

### 1

```{r}
km = kmeans(X, centers = 2, nstart = 10)
tab = table(km$cluster, y)
sum(diag(tab))/sum(tab)
```

Without knowing anything about the labels, the k-means looks like it has achieved an accuracy of 0.2602041. HOWEVER! we should compare this with the baseline accuracy where we just assign all observations to the largest group:

```{r}
tab = table(y)
tab
```

```{r}
max(tab)/sum(tab)
```

So if we just say all observations donâ€™t have diabetes our accuracy is 65%, so k-means hasnâ€™t done a particularly good job on differentiating by diabetes status, but thatâ€™s OK, we werenâ€™t asking it to do that, we were just asking it to find two similar groups in the data.

### 2

ðŸ¤” What is PCA?

ðŸ‘‰ **One summary sentence of PCA**

PCA reduces the dimension of data by creating **new** features (known as **Principal Components**) that *summarise* the original high-dimensional data using *all existing* variables in a way that those new features (1) maximise the variance in the original data and (2) are mutually uncorrelated.

Also, see [the GOATED PCA post](https://stats.stackexchange.com/questions/2691/making-sense-of-principal-component-analysis-eigenvectors-eigenvalues) on CrossValidated.

```{r}
pca_pima = princomp(X) # perform PCA
options(digits = 2)
summary(pca_pima)
```

The first two principal components account for 51% of the variance in the data.

```{r}
# install.packages('remotes')
# remotes::install_github('vqv/ggbiplot')
library(ggbiplot)

p1 = ggbiplot::ggscreeplot(pca_pima) + 
  theme_bw(base_size = 16) +
  labs(y = "Prop. of explained variance")

p2 = ggbiplot::ggscreeplot(pca_pima, type = "cev") + 
  theme_bw(base_size = 16) +
  labs(y = " Cumulative prop. of explained variance")

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

### 3

```{r}
# biplot(pca_pima) # base graphics works just fine
ggbiplot::ggbiplot(pca_pima, labels.size = 5, varname.size = 8, alpha = 0.2) +
    theme_bw()
```

```{r}
ggbiplot::ggbiplot(pca_pima, labels.size = 5, varname.size = 8, alpha = 0.25,
                   groups = factor(km$cluster)) + # group data points by k-means clustering classes
  theme_bw() + 
  labs(title = "k-means clustering") +
  scale_color_brewer(palette = "Set1") # color points differently according to k-means clustering classes
```

```{r}
ggbiplot::ggbiplot(pca_pima, labels.size = 5, varname.size = 8, alpha = 0.25,
                   groups = y) + 
  theme_bw() + 
  labs(title = "True classification") +
  scale_color_brewer(palette = "Set1")
```

Using the `factoextra` package which plots the two clusters in the space spanned by the first two principal components:

```{r}
# install.packages('factoextra')
library(factoextra)
fviz_cluster(km, data = X, geom = "point") + theme_classic()
```
