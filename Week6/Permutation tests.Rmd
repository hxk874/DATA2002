---
title: "Permutation tests"
date: "2024-09-02"
author: "Ellen Ebdrup"
output: 
  html_document: 
    ### IMPORTANT ###
    # self_contained: true # Creates a single HTML file as output
    code_folding: show # Code folding; allows you to show/hide code chunks
    ### USEFUL ###
    code_download: true # Includes a menu to download the code file
    ### OPTIONAL ###
    df_print: paged # Sets how dataframes are automatically printed
    theme: readable # Controls the font, colours, etc.
    toc: true # (Useful) Creates a table of contents!
    toc_float: true # table of contents at the side
    number_sections: false # (Optional) Puts numbers next to heading/subheadings
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Permutation tests

## Visual inference
Example of how different combinations of thee same data can be visualied. 

```{r}
library(nullabor) # you probably need to run: install.packages("nullabor")
library(ggplot2)
set.seed(1)
lineup_df = lineup(null_permute('mpg'), mtcars, pos = 10)
lineup_df |> ggplot() + 
  aes(x = mpg, y = wt) + 
  geom_point() + 
  facet_wrap(~ .sample) + theme_linedraw() + 
  theme(strip.text.x = element_text(size = 16, face = "bold")) + 
  labs(x = "Miles/(US) gallon", y = "Weight (1000 lbs)")
```

## Tea Lady example
### Permutations

We could also consider all 40,320 different orderings (permutations) of 8 cups of tea.
```{r}
# install.packages("arrangements")
library(arrangements)
permute_8 = permutations(8)
head(permute_8, 6) # look at the "first" 6 permutations
```

```{r}
tail(permute_8, 6) # look at the "last" 6 permutations
```

Use the `permuations()` function on the truth vector:
```{r}
truth = c("milk","tea","tea","milk","tea","tea","milk","milk")
permute_guess = permutations(truth)
permute_guess[92, ] # 92nd permutation
```


We can check if a particular sequence of tea cups is identical to the true sequence:
```{r}
identical(truth, truth)

identical(permute_guess[92,], truth) # this one shows that the Lady was not correct
```


### Exact p-value

We can calculate the exact p-value by looking across all permutations:
```{r}
B = nrow(permute_guess)
check_correct = vector("numeric", length = B)
for(i in 1:B) { # iterate over all rows and ask: are the prediction the same as the truth?
  check_correct[i] = identical(permute_guess[i,], truth)
}
c(sum(check_correct), mean(check_correct))
# mean(check_correct): avg num of times it was correct == Fisheres exact test P-value
```

The p-value is the same as we get using Fisher’s exact test!
```{r}
truth = c("milk", "tea", "tea", "milk", "tea", "tea", "milk", "milk")
predicted = c("milk", "tea", "tea", "milk", "tea", "tea", "milk", "milk")
tea_mat = table(truth, predicted)
fisher.test(tea_mat, alternative = "greater")$p.value # Same as mean(check_correct) from above
```


### Approximate p-value

Often it’s not feasible to consider all $n!$ permutations, so we can sample() a selection of them.
```{r}
set.seed(123)
truth = c("milk","tea","tea","milk","tea","tea","milk","milk")
B = 10000
result = vector(length = B) # initialise outside the loop
for(i in 1:B){
  # without replacement
  guess = sample(truth, size = 8, replace = FALSE) # does the permutation
  result[i] = identical(guess, truth)
}
mean(result) # result of just guessing
```
Which is pretty close to the exact p-value (= 0.01428)


# Permutation test: two independent samples

## Plant growth
The `PlantGrowth` data has results from an experiment to compare yields (as measured by dried weight of plants) obtained under a control and two different treatment conditions (Dobson, 1983, Table 7.1).

3 different treatment gruops (red, green, blue)

```{r}
# built into R, make it available
data("PlantGrowth") 
library(tidyverse)
PlantGrowth |> ggplot() +
  aes(y = weight, x = group, 
      colour = group) + 
  geom_boxplot(coef = 10) + 
  geom_jitter(width = 0.1, size = 5) + 
  theme(legend.position = "none") +
  labs(y = "Weight (g)", x = "Group")
```

We want to compare the control group to the treatment 2 group.
```{r}
(dat = PlantGrowth |> filter(group %in% c("ctrl", "trt2")))
```

### Checking for normality: Q-Q plot
reasonably close
Properly fine. 
two normal populations

```{r}
 dat |> 
  ggplot() + aes(sample = weight) + 
  geom_qq() + geom_qq_line() + 
  facet_grid(cols = vars(group), labeller = label_both) + 
  labs(y = "Weight (g)", x = "Standard normal quantiles")
```

### What do our “standard” methods say?

Thee Wilcoxon R-∑ test has a little bit of a larger p-value than the $t$-test: $0.063 > 0.047$. 

Two-sample t-test
```{r}
t.test(weight ~ group, data = dat, var.equal = TRUE)
```

Wilcoxon rank-sum test
```{r}
wilcox.test(weight ~ group, data = dat)
```

Extracting information from t.test objects
```{r}
(tt = t.test(weight ~ group, data = dat, var.equal = TRUE))

names(tt) # name of all the parameters/results from test.
tt$statistic # pick this one for the t statistic
```


### Permutation test



Permute class labels (many times) => we end up destroying any structure, there might have been in the org data set => we generate test statistic knowing that the $H_0$ is true and therefore that there is no difference between the treatment and control group. 

See what values we get for the t-test statistic.
```{r}
B = 10000 # number of permuted samples we will consider
permuted_dat = dat # make a copy of the data
t_null = vector("numeric", B) # initialise outside loop
for(i in 1:B) {
  # the group col is a sample from org dataset
  permuted_dat$group = sample(dat$group) # this does the permutation = resampling the group
  # shuffling thee labels => no diff the control and the treatment because fo the randomness in suffling
  t_null[i] = t.test(weight ~ group, data = permuted_dat)$statistic
}
```

```{r}
t_null |> 
  data.frame() |> 
  ggplot() + 
  aes(x = t_null) + 
  geom_histogram(binwidth = 0.15) + 
  labs(x = "Test statistics from permuted samples")
```

```{r}
data.frame(abs_t_null = abs(t_null)) |> 
  ggplot() + 
  aes(x = abs_t_null) +
  geom_histogram(binwidth = 0.1,
                 boundary = 0) + 
  geom_vline(
    xintercept = abs(tt$statistic), 
    col = "red", lwd = 2) + 
  labs(
    x = "Absolute value of test statistic"
  )
```

What proportion of test statistics from randomly permuted data are more extreme than the test statistic we observed?
```{r}
mean(abs(t_null) >= abs(tt$statistic)) # This is our permutation test p-value.
```

 
## Latent heat of fusion

Natrella (1963, pp. 3–23) presents data from two methods that were used in a study of the latent heat of fusion of ice. Both method A (digital method) and method B (method of mixtures) were conducted with the specimens cooled to -0.72°C. The data represent the change in total heat from -0.72°C to water at 0°C, in calories per gram of mass.

• Does the data support the hypothesis that the electrical method (method A) gives larger results?

```{r}
A = c(79.98, 80.04, 80.02, 80.04, 80.03, 80.03, 80.04, 
      79.97, 80.05, 80.03, 80.02, 80.00, 80.02)
B = c(80.02, 79.94, 79.98, 79.97, 79.97, 80.03, 79.95, 
      79.97)
heat = data.frame(energy = c(A,B),
  method = rep(c("A","B"), c(length(A), length(B)))
)
```



```{r}
heat |> ggplot() + 
  aes(x = method, y = energy) + 
  geom_boxplot(coef = 10) + 
  geom_dotplot(stackdir = "center", binaxis = "y") +
  labs(y = "Heat of fusion (cal/g)", x = "Method")
```

We havenot assumed equal variance
```{r}
tt = t.test(energy ~ method, data = heat, alternative = "greater")
tt

t0_original = tt$statistic
t0_original
```

• How many permutations of the class label are there?
```{r}
n = nrow(heat)
n

factorial(n)

#english::words(factorial(n))
```
Fifty one quintillion ninety quadrillion nine hundred forty-two trillion one hundred seventy-one billion seven hundred nine million four hundred forty thousand

### Permutation test p-value
```{r}
B = 10000 # number of permuted samples we will consider
permuted_heat = heat # make a copy of the data
t_null = vector("numeric", B) # initialise outside loop
for(i in 1:B) {
  permuted_heat$method = sample(heat$method) # this does the permutation
  t_null[i] = t.test(energy ~ method, data = permuted_heat)$statistic
}
mean(t_null>=t0_original)
```

• Why didn’t we need to specify alternative = "greater" in the t.test() function?
because we don't need to worry about the P-value under the hood. We are just interested in the test statistic


```{r}
perm_heat = heat 
perm_heat$id = 1
for(i in 2:6){
  temp = heat
  temp$method = sample(temp$method)
  temp$id = i
  perm_heat = rbind(perm_heat, temp)
}
perm_heat |> 
  group_by(id) |> 
  summarise(
    t_stat = t.test(
      energy[method=="A"], 
      energy[method=="B"])$statistic
  )
```

What is going on under the hood?
```{r}
perm_heat |> ggplot() +
  aes(x = method, y = energy) + 
  geom_boxplot(coef = 10) + 
  scale_y_continuous(n.breaks = 3) + 
  facet_wrap(vars(id), ncol = 6) + 
  labs(y = "Heat of fusion (cal/g)",
       x = "Method")
```



```{r}
t_null |> data.frame() |> ggplot() + aes(x = t_null) + 
  geom_histogram(alpha=0.5) + 
  geom_vline(xintercept = t0_original, colour = "red", linetype = "dashed") + 
  geom_text(aes(x = t0_original, label = "Original test statistic", y = Inf), 
            colour = "red", angle = 90, hjust = 1, vjust = -1, size = 7) + 
  labs(x = "Test statistics from permuted samples", y = "Count")
```

### What about outliers?
What happens if there is an outlier in the data?

# pov: there is a big outlier 80.2 : see plot below
Below we change the first value for the B method from 80.02 to 80.20.

```{r}
heat1 = heat
heat1$energy[14]

# edit 80.02 -> 80.20 to introduce an outlier
heat1$energy[14] = 80.20 
```

```{r}
ggplot(heat1) + 
  aes(x = method, y = energy) + 
  geom_boxplot() + 
  geom_dotplot(stackdir = "center",
               binaxis = "y") +
  labs(y = "Heat of fusion (cal/g)",
       x = "Method")
```


### Our “standard” test results

The more roobust Wilcoxon test tell us that we shout reject $H_0$: 0.019 < 0.05
```{r}
wilcox.test(energy ~ method, data = heat1, 
            alternative = "greater", correct = FALSE)
```
The two-sample t-test is no longer rejecting the $H_0$ hypothesis: 0.27 > 0.05
```{r}
t.test(energy ~ method, data = heat1, alternative = "greater")
```

### Permutation test using the Wilcoxon rank-sum statistic

```{r}
t0_original = wilcox.test(energy ~ method, data = heat1)$statistic
set.seed(1234)
B = 10000
permuted_heat1 = heat1
t_null = vector("numeric", B)
for(i in 1:B){
  permuted_heat1$method = sample(heat1$method)
  t_null[i] = wilcox.test(energy ~ method, data = permuted_heat1, verbose=False)$statistic
}
mean(t_null >= t0_original)
```



```{r}
t_null |> data.frame() |> ggplot() + aes(x = t_null) + 
  geom_histogram(alpha=0.5) + 
  geom_vline(xintercept = t0_original, colour = "red", linetype = "dashed") + 
  geom_text(aes(x = t0_original, label = "Original test statistic", y = Inf), 
            colour = "red", angle = 90, hjust = 1, vjust = -1, size = 7) + 
  labs(x = "Test statistics from permuted samples", y = "Count")
```

